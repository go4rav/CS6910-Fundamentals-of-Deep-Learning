{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G5gA0Y4f8NOG"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist as data\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hsi1qyMmwwJj",
    "outputId": "cbbe0f31-a52b-4407-c085-8cec713f083d"
   },
   "outputs": [],
   "source": [
    "!pip install wandb\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bNwTSrbL1NL4"
   },
   "outputs": [],
   "source": [
    "# Flattening input training data each image of size 28x28 pixels into a vector of size 784\n",
    "def FlattenInput(X_train, X_test):\n",
    "  num_train = X_train.shape[0]\n",
    "  num_test = X_test.shape[0]\n",
    "  \n",
    "  features = X_train.shape[1]*X_train.shape[2]  # 28x28 = 784\n",
    "  X_train=X_train.reshape(num_train, features)\n",
    "  X_test=X_test.reshape(num_test,features)\n",
    "  \n",
    "\n",
    "  X_train=np.transpose(X_train)\n",
    "  X_test = np.transpose(X_test)\n",
    "  \n",
    "\n",
    "  X_train = X_train/255  # normalised data\n",
    "  X_test = X_test/255\n",
    " \n",
    "\n",
    "  return(X_train, X_test)\n",
    "\n",
    "# One hot encoding of output labels of training data\n",
    "def OneHotEncoding(Y_train,num_train):\n",
    "  Y_train_orig= Y_train[:,:]\n",
    "\n",
    "\n",
    "  Y_train=np.zeros([10,num_train])\n",
    "\n",
    "  for i in range(num_train):\n",
    "    index=Y_train_orig[0,i]\n",
    "    Y_train[index,i]=1\n",
    "\n",
    "  return(Y_train, Y_train_orig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kcmg2eahorl"
   },
   "outputs": [],
   "source": [
    "# Shuffles  training data\n",
    "def ShuffleData(X_train, Y_train):\n",
    "  m=X_train.shape[1]\n",
    "  permutation = list(np.random.permutation(m))\n",
    "  X_train = X_train[:, permutation]\n",
    "  Y_train = Y_train[:, permutation]\n",
    "  return(X_train,Y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Divides input training data into mini batches of given batch size\n",
    "def GetMiniBatches(X_train, Y_train,mini_batch_size):\n",
    "    m=X_train.shape[1]  \n",
    "    num_batches = m//mini_batch_size\n",
    "    X_mini_batches = []\n",
    "    Y_mini_batches = []\n",
    "\n",
    "    #X_train, Y_train = ShuffleData(X_train, Y_train)\n",
    "\n",
    "    for i in range(num_batches):\n",
    "      x=X_train[:,i*mini_batch_size:(i+1)*(mini_batch_size)]\n",
    "      y=Y_train[:,i*mini_batch_size:(i+1)*(mini_batch_size)]\n",
    "      X_mini_batches.append(x)\n",
    "      Y_mini_batches.append(y)\n",
    "\n",
    "    if m%mini_batch_size!=0:\n",
    "      index = num_batches*mini_batch_size\n",
    "      x=X_train[:,index:index+m%mini_batch_size]\n",
    "      y=Y_train[:,index:index+m%mini_batch_size]\n",
    "      X_mini_batches.append(x)\n",
    "      Y_mini_batches.append(y)\n",
    "    \n",
    "    \n",
    "    return(X_mini_batches, Y_mini_batches)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5qjwu7m0NTt"
   },
   "outputs": [],
   "source": [
    "# Activations and their derivatives\n",
    "\n",
    "\n",
    "# Computes Relu activation function\n",
    "def Relu(Z):\n",
    "\n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    return A\n",
    "\n",
    "# Computes derivative of Relu Activation\n",
    "def ReluGradient(z):\n",
    "    dZ = np.zeros(z.shape) \n",
    "    dZ[z > 0] = 1\n",
    "    assert (dZ.shape == z.shape)\n",
    "    return dZ\n",
    "\n",
    "\n",
    "# Computes Sigmoid activation function\n",
    "def Sigmoid(z):\n",
    "    a=1/(1+np.exp(-z))\n",
    "    return(a)\n",
    "\n",
    "\n",
    "# Computes derivative of Sigmoid Activation\n",
    "def SigmoidGradient(z):\n",
    "    a=Sigmoid(z)\n",
    "    return(a*(1-a))\n",
    "\n",
    "# Computes Tanh activation function\n",
    "def Tanh(z):\n",
    "  a=np.tanh(z)\n",
    "  return(a)\n",
    "\n",
    "# Computes derivative of Tanh Activation\n",
    "def TanhGradient(z):\n",
    "  a=Tanh(z)\n",
    "  return(1-a**2)\n",
    "\n",
    "#  Computes Softmax activation function\n",
    "def Softmax(z):\n",
    "    num=np.exp(z)\n",
    "    den=np.sum(np.exp(z),axis=0)\n",
    "    a=num/den\n",
    "    return(a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rzk5ZpZqBEir"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Initialize weights using Xavier or Random weight initialisations\n",
    "def InitializeWeights(layers, params, M, R, initialiser):\n",
    "  seed=3\n",
    "  np.random.seed(seed)\n",
    "  for i in range(1,len(layers)):\n",
    "      if(initialiser==\"xavier\"):\n",
    "        params[\"W\"+str(i)]=np.random.randn(layers[i],layers[i-1])*np.sqrt(2 / (layers[i]+layers[i-1]))\n",
    "      elif(initialiser==\"random\"):\n",
    "        params[\"W\"+str(i)]=np.random.normal(2,1,[layers[i],layers[i-1]])\n",
    "        \n",
    "\n",
    "      params[\"b\"+str(i)]=np.zeros([layers[i],1])\n",
    "      M[\"W\"+str(i)]=np.zeros([layers[i],layers[i-1]])\n",
    "      M[\"b\"+str(i)]=np.zeros([layers[i],1])\n",
    "      R[\"W\"+str(i)]=np.zeros([layers[i],layers[i-1]])\n",
    "      R[\"b\"+str(i)]=np.zeros([layers[i],1])\n",
    "  return(params, M, R)\n",
    "\n",
    "\n",
    "# computes loss through cross entropy error function\n",
    "def CrossEntropyError(a,Y,params,layers,weight_decay):\n",
    "    m=a.shape[1]\n",
    "    error=-(np.sum(np.sum(Y*np.log(a),axis=1),axis=0))\n",
    "    #error=-np.sum(Y*np.log(a)+(1-Y)*np.log(1-a))/m\n",
    "    for i in range(1,len(layers)):\n",
    "      regu_cost = (weight_decay/2)*np.sum(np.sum(np.square(params[\"W\"+str(i)]),axis=1),axis=0)\n",
    "    error+=regu_cost\n",
    "    return(error)\n",
    "\n",
    "\n",
    "# computes loss through Mean Squared Error Function   \n",
    "def MeanSquaredError(a,Y,params,layers,weight_decay):\n",
    "    m=a.shape[1]\n",
    "    error= np.sum(np.sum(np.square(a-Y),axis=1),axis=0)/2\n",
    "    for i in range(1,len(layers)):\n",
    "      regu_cost = (weight_decay/2)*np.sum(np.sum(np.square(params[\"W\"+str(i)]),axis=1),axis=0)\n",
    "    error+=regu_cost\n",
    "    return(error)\n",
    "\n",
    "\n",
    "# Computes pre-activations and activations through forward propagation\n",
    "def ForwardPropagation(params,layers,X,activation):\n",
    "    \n",
    "    a=X\n",
    "    n=len(layers)-1\n",
    "    Z=[]\n",
    "    A=[]\n",
    "    A.append(X)\n",
    "    Z.append(X)\n",
    "    for i in range(1,len(layers)-1):\n",
    "        W=params[\"W\"+str(i)]\n",
    "        b=params[\"b\"+str(i)]\n",
    "        z = np.dot(W,a)+b\n",
    "        if(activation==\"relu\"):\n",
    "          a = Relu(z)\n",
    "        elif(activation==\"sigmoid\"):\n",
    "          a = Sigmoid(z)\n",
    "        elif(activation==\"tanh\"):\n",
    "          a = Tanh(z)\n",
    "        A.append(a)\n",
    "        Z.append(z)\n",
    "    W=params[\"W\"+str(n)]\n",
    "    b=params[\"b\"+str(n)]\n",
    "    z = np.dot(W,a)+b\n",
    "    a = Softmax(z)\n",
    "    Z.append(z)\n",
    "    A.append(a)\n",
    "    return(A,Z)\n",
    "\n",
    "\n",
    "# Computes gradients of weights through backward propagation\n",
    "def BackwardPropagation(params, layers, Z, A, learning_rate, Y, activation, loss_function):\n",
    "    m=Y.shape[1]\n",
    "    l=len(layers)-1\n",
    "    if(loss_function=='cross-entropy'):\n",
    "        dz= A[l]-Y\n",
    "    elif(loss_function=='MSE'):\n",
    "        dz= np.multiply((A[l]- Y), np.multiply(A[l], (1 - A[l])))\n",
    "      \n",
    "    gradients={}\n",
    "    while(l>=0):\n",
    "        dw = np.dot(dz,np.transpose(A[l-1]))/m\n",
    "        db = np.sum(dz,axis=1)/m \n",
    "        db=db.reshape(db.shape[0],1)\n",
    "        gradients[\"dw\"+str(l)]=dw\n",
    "        gradients[\"db\"+str(l)]=db\n",
    "        if(l>=2):\n",
    "            da= np.dot(np.transpose(params[\"W\"+str(l)]), dz)\n",
    "            if(activation==\"relu\"):\n",
    "              dz = da*ReluGradient(Z[l-1])\n",
    "            elif(activation==\"sigmoid\"):\n",
    "              dz = da*SigmoidGradient(Z[l-1])\n",
    "            elif(activation==\"tanh\"):\n",
    "              dz = da*TanhGradient(Z[l-1])\n",
    "        l=l-1\n",
    "    return(gradients)\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9EruJ95G0-E"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Updates weights through Stochastic Gradient Descent\n",
    "def UpdateWeightsSGD(m, params,gradients,layers, learning_rate, weight_decay):\n",
    "    for i in range(1,len(layers)):\n",
    "        params[\"W\"+str(i)]-= learning_rate*gradients[\"dw\"+str(i)]\n",
    "        params[\"b\"+str(i)]-= learning_rate*gradients[\"db\"+str(i)]\n",
    "        params[\"W\"+str(i)]-= learning_rate*(weight_decay/m)*params[\"W\"+str(i)]\n",
    "        params[\"b\"+str(i)]-= learning_rate*(weight_decay/m)*params[\"b\"+str(i)]\n",
    "    return(params)\n",
    "\n",
    "\n",
    "# Updates weights through Momentum Gradient Descent\n",
    "def UpdateWeightsMomentum(m, params, gradients, M, layers, learning_rate,weight_decay, beta1):\n",
    "    for i in range(1,len(layers)):\n",
    "        M[\"W\"+str(i)]=beta1*M[\"W\"+str(i)]+(1-beta1)*gradients[\"dw\"+str(i)]\n",
    "        M[\"b\"+str(i)]=beta1*M[\"b\"+str(i)]+(1-beta1)*gradients[\"db\"+str(i)]\n",
    "        params[\"W\"+str(i)]-= learning_rate* M[\"W\"+str(i)]\n",
    "        params[\"b\"+str(i)]-= learning_rate*M[\"b\"+str(i)]\n",
    "        params[\"W\"+str(i)]-= learning_rate*(weight_decay/m)*params[\"W\"+str(i)]\n",
    "        params[\"b\"+str(i)]-= learning_rate*(weight_decay/m)*params[\"b\"+str(i)]\n",
    "    return(params,M)\n",
    "\n",
    "\n",
    "# Updates weights through RMSprop Gradient Descent\n",
    "def UpdateWeightsRMS(m, params, gradients,R, layers, learning_rate, weight_decay, beta1 ,eps):\n",
    "\n",
    "    for i in range(1,len(layers)):\n",
    "        R[\"W\"+str(i)]=beta1*R[\"W\"+str(i)]+(1-beta1)*np.power(gradients[\"dw\"+str(i)],2)\n",
    "        R[\"b\"+str(i)]=beta1*R[\"b\"+str(i)]+(1-beta1)*np.power(gradients[\"db\"+str(i)],2)\n",
    "        params[\"W\"+str(i)]-= (learning_rate*gradients[\"dw\"+str(i)])/(np.sqrt(R[\"W\"+str(i)])+eps)\n",
    "        params[\"b\"+str(i)]-= (learning_rate*gradients[\"db\"+str(i)])/(np.sqrt(R[\"b\"+str(i)])+eps)\n",
    "        params[\"W\"+str(i)]-= learning_rate*(weight_decay/m)*params[\"W\"+str(i)]\n",
    "        params[\"b\"+str(i)]-= learning_rate*(weight_decay/m)*params[\"b\"+str(i)]\n",
    "    return(params,R)\n",
    "\n",
    "\n",
    "# Updates weights through Nesterov Accelerated Gradient Descent\n",
    "def UpdateWeightsNesterov(m, params, lookahead_grads, M, layers, learning_rate, weight_decay, beta1):\n",
    "    \n",
    "    for i in range(1,len(layers)):\n",
    "        M[\"W\"+str(i)]=beta1*M[\"W\"+str(i)]+(1-beta1)*lookahead_grads[\"dw\"+str(i)]\n",
    "        M[\"b\"+str(i)]=beta1*M[\"b\"+str(i)]+(1-beta1)*lookahead_grads[\"db\"+str(i)]\n",
    "        params[\"W\"+str(i)]-= learning_rate* M[\"W\"+str(i)]\n",
    "        params[\"b\"+str(i)]-= learning_rate*M[\"b\"+str(i)]\n",
    "        params[\"W\"+str(i)]-= learning_rate*(weight_decay/m)*params[\"W\"+str(i)]\n",
    "        params[\"b\"+str(i)]-= learning_rate*(weight_decay/m)*params[\"b\"+str(i)]\n",
    "    return(params,M)\n",
    "\n",
    "# Updates weights through Nesterov Adam Gradient Descent\n",
    "def UpdateWeightsNAdam(m, params, lookahead_grads, layers, M, R, learning_rate, weight_decay, beta1, beta2, eps, t):\n",
    "    M_c={}\n",
    "    R_c={}\n",
    "    for i in range(1,len(layers)):\n",
    "        M[\"W\"+str(i)]=beta1*M[\"W\"+str(i)]+(1-beta1)*lookahead_grads[\"dw\"+str(i)]\n",
    "        M[\"b\"+str(i)]=beta1*M[\"b\"+str(i)]+(1-beta1)*lookahead_grads[\"db\"+str(i)]\n",
    "\n",
    "        M_c[\"W\"+str(i)]=M[\"W\"+str(i)]/(1-np.power(beta1,t))   # bias correction\n",
    "        M_c[\"b\"+str(i)]=M[\"b\"+str(i)]/(1-np.power(beta1,t))\n",
    "        R[\"W\"+str(i)]=beta2*R[\"W\"+str(i)]+(1-beta2)*np.power(lookahead_grads[\"dw\"+str(i)],2)\n",
    "        R[\"b\"+str(i)]=beta2*R[\"b\"+str(i)]+(1-beta2)*np.power(lookahead_grads[\"db\"+str(i)],2)\n",
    "\n",
    "        R_c[\"W\"+str(i)]=R[\"W\"+str(i)]/(1-np.power(beta2,t))     # bias correction\n",
    "        R_c[\"b\"+str(i)]=R[\"b\"+str(i)]/(1-np.power(beta2,t))\n",
    "        params[\"W\"+str(i)]-= (learning_rate*M_c[\"W\"+str(i)])/(np.sqrt(R_c[\"W\"+str(i)])+eps)\n",
    "        params[\"b\"+str(i)]-= (learning_rate*M_c[\"b\"+str(i)])/(np.sqrt(R_c[\"b\"+str(i)])+eps)\n",
    "        params[\"W\"+str(i)]-= learning_rate*(weight_decay/m)*params[\"W\"+str(i)]\n",
    "        params[\"b\"+str(i)]-= learning_rate*(weight_decay/m)*params[\"b\"+str(i)]\n",
    "        \n",
    "    return(params,M,R)\n",
    "\n",
    "# Updates weights through Adam Gradient Descent\n",
    "def UpdateWeightsAdam(m, params, gradients, layers, M, R, learning_rate, weight_decay, beta1, beta2, eps, t):\n",
    "    M_c={}\n",
    "    R_c={}\n",
    "    for i in range(1,len(layers)):\n",
    "        M[\"W\"+str(i)]=beta1*M[\"W\"+str(i)]+(1-beta1)*gradients[\"dw\"+str(i)]\n",
    "        M[\"b\"+str(i)]=beta1*M[\"b\"+str(i)]+(1-beta1)*gradients[\"db\"+str(i)]\n",
    "\n",
    "        M_c[\"W\"+str(i)]=M[\"W\"+str(i)]/(1-np.power(beta1,t))   # bias correction\n",
    "        M_c[\"b\"+str(i)]=M[\"b\"+str(i)]/(1-np.power(beta1,t))\n",
    "        R[\"W\"+str(i)]=beta2*R[\"W\"+str(i)]+(1-beta2)*np.power(gradients[\"dw\"+str(i)],2)\n",
    "        R[\"b\"+str(i)]=beta2*R[\"b\"+str(i)]+(1-beta2)*np.power(gradients[\"db\"+str(i)],2)\n",
    "\n",
    "        R_c[\"W\"+str(i)]=R[\"W\"+str(i)]/(1-np.power(beta2,t))    # bias correction\n",
    "        R_c[\"b\"+str(i)]=R[\"b\"+str(i)]/(1-np.power(beta2,t))\n",
    "        params[\"W\"+str(i)]-= (learning_rate*M_c[\"W\"+str(i)])/(np.sqrt(R_c[\"W\"+str(i)])+eps)\n",
    "        params[\"b\"+str(i)]-= (learning_rate*M_c[\"b\"+str(i)])/(np.sqrt(R_c[\"b\"+str(i)])+eps)\n",
    "        params[\"W\"+str(i)]-= learning_rate*(weight_decay/m)*params[\"W\"+str(i)]\n",
    "        params[\"b\"+str(i)]-= learning_rate*(weight_decay/m)*params[\"b\"+str(i)]\n",
    "    return(params,M,R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sms3y9Ma0LoE"
   },
   "outputs": [],
   "source": [
    "# predicts training data accuracy, validation data accuracy, test data accuracy of given model\n",
    "def Predict(params, layers, X_train, X_test, Y_train_orig, Y_test,activation,weight_decay):\n",
    "  m=X_validation.shape[1]\n",
    "  A,Z=ForwardPropagation(params, layers, X_validation,activation)\n",
    "  val_error = CrossEntropyError(A[-1], Y_validation,params,layers,weight_decay)\n",
    "  val_error = val_error/m\n",
    "\n",
    "  num_train = X_train.shape[1]\n",
    "  num_test = X_test.shape[1]\n",
    "  num_validation = X_validation.shape[1]\n",
    "  \n",
    "  #predicts training data accuracy\n",
    "  A,Z=ForwardPropagation(params, layers, X_train, activation)\n",
    "  pred=A[-1]\n",
    "  max_index = np.argmax(pred, axis=0)\n",
    "  count=0\n",
    "  for i in range(num_train):\n",
    "      if(Y_train_orig[0,i]==max_index[i]):\n",
    "          count+=1\n",
    "  train_accuracy = (count/num_train)*100\n",
    "\n",
    "  #predicts validation data accuracy\n",
    "  A,Z=ForwardPropagation(params, layers, X_validation, activation)\n",
    "  pred=A[-1]\n",
    "  max_index = np.argmax(pred, axis=0)\n",
    "  count=0\n",
    "  for i in range(num_validation):\n",
    "      if(Y_validation_orig[0,i]==max_index[i]):\n",
    "          count+=1\n",
    "\n",
    "  Y_pred=max_index\n",
    "\n",
    "  \n",
    "\n",
    "  val_accuracy = (count/num_validation)*100\n",
    "\n",
    "  #predicts testing data accuracy\n",
    "  A,Z=ForwardPropagation(params, layers, X_test,activation)\n",
    "  pred=A[-1]\n",
    "  max_index = np.argmax(pred, axis=0)\n",
    "  count=0\n",
    "  for i in range(num_test):\n",
    "      if(Y_test[0,i]==max_index[i]):\n",
    "          count+=1\n",
    "\n",
    "  test_accuracy = (count/num_test)*100\n",
    "\n",
    "  return(val_error,train_accuracy,val_accuracy,test_accuracy,Y_pred)\n",
    "\n",
    "  \n",
    "          \n",
    "      \n",
    "# creates confusion matrix\n",
    "\n",
    "def CreateConfusionMatrix(Y_validation_orig, Y_pred):\n",
    "  Y_true = Y_validation_orig[0,:]\n",
    "  wandb.log({\"confusion matrix\" : wandb.plot.confusion_matrix(\n",
    "                        probs=None,\n",
    "                        y_true=Y_true,\n",
    "                        preds=Y_pred,\n",
    "                        class_names=[\"Top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"])})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zsaDS2Ala_k9",
    "outputId": "62f675c3-abac-4bee-be6c-21f302bd8108"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-91nnO5y-yUr",
    "outputId": "ffb93404-355f-4393-b7b4-fe0a2be7a9b1"
   },
   "outputs": [],
   "source": [
    "# configure sweep parameters\n",
    "sweep_config = {\n",
    "  \"name\": \"Bayesian Sweep\",\n",
    "  \"method\": \"bayes\",\n",
    "  \"metric\":{\n",
    "  \"name\": \"val_accuracy\",\n",
    "  \"goal\": \"maximize\"\n",
    "  },\n",
    "  \"parameters\": {\n",
    "        \"epochs\": {\n",
    "            \"values\": [5, 10]\n",
    "        },\n",
    "\n",
    "        \"initialiser\": {\n",
    "            \"values\": [\"random\", \"xavier\"]\n",
    "        },\n",
    "\n",
    "        \"hidden_layers\": {\n",
    "            \"values\": [3,4,5]\n",
    "        },\n",
    "        \n",
    "        \n",
    "        \"size_hidden_layer\": {\n",
    "             \"values\": [32, 64, 128]\n",
    "\n",
    "        },\n",
    "        \n",
    "        \"activation\": {\n",
    "            \"values\": [ 'sigmoid', 'tanh','relu']\n",
    "        },\n",
    "        \n",
    "        \"learning_rate\": {\n",
    "             \"values\": [0.001, 0.0001]\n",
    "        },\n",
    "        \n",
    "        \n",
    "        \"weight_decay\": {\n",
    "            \"values\": [0, 0.0005,0.5]\n",
    "        },\n",
    "        \n",
    "        \"optimiser\": {\n",
    "             \"values\": [\"sgd\", \"momentum\", \"nesterov\", \"rmsprop\", \"adam\", \"nadam\"]\n",
    "        },\n",
    "                    \n",
    "        \"batch_size\": {\n",
    "             \n",
    "             \"values\": [16,32,64]\n",
    "        }\n",
    "        \n",
    "        \n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config,project='CS6910 Assignment 1', entity='go4rav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yAuAvnAdi4Hm"
   },
   "outputs": [],
   "source": [
    "# Main Component for training our model\n",
    "\n",
    "def TrainModel():\n",
    "    # Default sweep parameters\n",
    "    config_defaults = dict(\n",
    "            epochs=10,\n",
    "            hidden_layers=3,\n",
    "            size_hidden_layer=32,\n",
    "            weight_decay=0,\n",
    "            learning_rate=1e-3,\n",
    "            optimiser=\"nadam\",\n",
    "            batch_size=16,\n",
    "            activation=\"relu\",\n",
    "            initialiser=\"xavier\",\n",
    "        )\n",
    "        \n",
    "    \n",
    "    wandb.init(config = config_defaults)\n",
    "    CONFIG=wandb.config\n",
    "    print(CONFIG)\n",
    "    hidden_layers=CONFIG.hidden_layers\n",
    "    size_hidden_layer=CONFIG.size_hidden_layer\n",
    "    learning_rate=CONFIG.learning_rate\n",
    "    weight_decay=CONFIG.weight_decay\n",
    "    optimiser=CONFIG.optimiser\n",
    "    activation=CONFIG.activation\n",
    "    epochs=CONFIG.epochs\n",
    "    mini_batch_size=CONFIG.batch_size\n",
    "    initialiser=CONFIG.initialiser\n",
    "\n",
    "    \n",
    "    loss_function='cross-entropy'\n",
    "    layers=[X_train.shape[0]]\n",
    "    for i in range(hidden_layers):\n",
    "      layers.append(size_hidden_layer)\n",
    "    layers.append(Y_train.shape[0])\n",
    "    params={}\n",
    "    M={}\n",
    "    R={}\n",
    "    lookahead_params={}\n",
    "\n",
    "    wandb.run.name = \"hl_\" + str(wandb.config.hidden_layers) +\"_hn_\" + str(wandb.config.size_hidden_layer)  + \"_opt_\" + wandb.config.optimiser +\"_lr_\" + str(wandb.config.learning_rate)+ \"_init_\" + wandb.config.initialiser +\"_bs_\"+str(wandb.config.batch_size)+\"_ac_\" + wandb.config.activation+ \"_wd_\"+str(weight_decay)\n",
    "\n",
    "    \n",
    "    # initialise weights\n",
    "    params, M, R = InitializeWeights(layers, params, M, R,initialiser)\n",
    "    (X_mini_batches, Y_mini_batches) = GetMiniBatches(X_train, Y_train,mini_batch_size)\n",
    "\n",
    "    epoch=1\n",
    "    beta1=0.9  # hyperparameter for exponentially weighted averages of momentum\n",
    "    beta2=0.999  # hyperparameter for exponentially weighted average of  rms\n",
    "    eps=1e-8     # small real value so that denominator do not become zero\n",
    "   \n",
    "    m=X_train.shape[1]\n",
    "    \n",
    "    t=0\n",
    "\n",
    "    while(epoch<=epochs):\n",
    "        train_error=0\n",
    "        for i in range(len(X_mini_batches)):\n",
    "          X_train_mini=X_mini_batches[i]\n",
    "          Y_train_mini=Y_mini_batches[i]\n",
    "\n",
    "          # Forward Propagation\n",
    "          A,Z=ForwardPropagation(params, layers, X_train_mini,activation)\n",
    "\n",
    "          # Computes Loss function\n",
    "          if(loss_function == \"cross-entropy\"):\n",
    "            train_error += CrossEntropyError(A[-1], Y_train_mini,params,layers,weight_decay)\n",
    "          elif(loss_function == \"MSE\"):\n",
    "            train_error += MeanSquaredError(A[-1],Y_train_mini,params,layers,weight_decay)\n",
    "          \n",
    "          \n",
    "          if(optimiser==\"nadam\" or optimiser==\"nesterov\"):\n",
    "\n",
    "              # Initialising lookaheads for nadam and nesterov\n",
    "              for i in range(1,len(layers)):\n",
    "                lookahead_params[\"W\"+str(i)]=params[\"W\"+str(i)]-M[\"W\"+str(i)]\n",
    "                lookahead_params[\"b\"+str(i)]=params[\"b\"+str(i)]-M[\"b\"+str(i)]\n",
    "\n",
    "              # Back propagation for nada and nesterov\n",
    "              lookahead_grads=BackwardPropagation(lookahead_params, layers,Z, A, learning_rate, Y_train_mini,activation,loss_function)\n",
    "\n",
    "              # Updating weights\n",
    "              if(optimiser==\"nesterov\"):\n",
    "                params, M = UpdateWeightsNesterov(num_train, params, lookahead_grads, M, layers, learning_rate, weight_decay, beta1)\n",
    "              elif(optimiser==\"nadam\"):\n",
    "                t=t+1\n",
    "                params,M,R = UpdateWeightsNAdam(num_train, params, lookahead_grads, layers, M, R, learning_rate, weight_decay, beta1,beta2,eps,t)\n",
    "\n",
    "          else:\n",
    "              # Back propagation sgd, momentum, rmsprop , adam\n",
    "              gradients=BackwardPropagation(params, layers, Z , A, learning_rate, Y_train_mini, activation, loss_function)\n",
    "\n",
    "              # updating weights\n",
    "              if(optimiser==\"sgd\"):\n",
    "                params = UpdateWeightsSGD(num_train, params,gradients,layers, learning_rate, weight_decay)\n",
    "              elif(optimiser==\"momentum\"):\n",
    "                params,M = UpdateWeightsMomentum(num_train, params, gradients, M, layers, learning_rate,weight_decay, beta1)\n",
    "              elif(optimiser==\"rmsprop\"):\n",
    "                params, R = UpdateWeightsRMS(num_train, params, gradients, R, layers, learning_rate, weight_decay, beta1 ,eps)\n",
    "              elif(optimiser==\"adam\"):\n",
    "                t=t+1\n",
    "                params, M, R = UpdateWeightsAdam(num_train,params, gradients, layers, M, R, learning_rate, weight_decay, beta1, beta2, eps, t)\n",
    "\n",
    "\n",
    "        # average training error\n",
    "        train_error=train_error/m\n",
    "\n",
    "        # predicts training data accuracy, validation data accuracy, test data accuracy of given model\n",
    "        val_error, train_accuracy, val_accuracy, test_accuracy, Y_pred = Predict(params, layers, X_train, X_test, Y_train_orig, Y_test,activation, weight_decay)\n",
    "\n",
    "        # plotting the sweeps\n",
    "        wandb.log({'train_loss':train_error, 'train_accuracy':train_accuracy, 'val_loss': val_error, 'val_accuracy':val_accuracy, 'epoch':epoch })\n",
    "        epoch+=1\n",
    "        print(\"epoch: \"+str(10), \"training error: \"+str(train_error), \"training accuracy: \"+str(train_accuracy), \"validation error: \"+str(val_error), \"validation accuracy: \"+str(val_accuracy))\n",
    "        \n",
    "    #predicts training data accuracy, validation data accuracy, test data accuracy of given model\n",
    "    val_error, train_accuracy, val_accuracy, test_accuracy, Y_pred = Predict(params, layers, X_train, X_test, Y_train_orig, Y_test,activation, weight_decay)\n",
    "\n",
    "    # plots confusion matrix\n",
    "    CreateConfusionMatrix(Y_validation_orig, Y_pred)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0TgTG9ZM9CI"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Loading Fashion MNIST dataset\n",
    "(X, Y), (X_test, Y_test) = data.load_data()\n",
    "\n",
    "# Flattening of Input data\n",
    "(X, X_test) = FlattenInput(X, X_test)\n",
    "\n",
    "Y = Y.reshape(1, X.shape[1])\n",
    "Y_test= Y_test.reshape(1, X_test.shape[1])\n",
    "\n",
    "\n",
    "# Shuffling of training data\n",
    "X,Y = ShuffleData(X, Y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Splitting input training data into training data and validation data in the ratio of 9:1\n",
    "num_train= int(0.9*X.shape[1])\n",
    "num_validation = int(0.1*X.shape[1])\n",
    "num_test=X_test.shape[1]\n",
    "\n",
    "\n",
    "\n",
    "X_train = X[:,:num_train]\n",
    "X_validation = X[:,num_train:]\n",
    "\n",
    "Y_train = Y[:,:num_train]\n",
    "Y_validation =  Y[:,num_train:]\n",
    "\n",
    "\n",
    "# One Hot Encoding of output labels of training data\n",
    "(Y_train,Y_train_orig)= OneHotEncoding(Y_train,num_train)\n",
    "\n",
    "# One Hot Encoding of output labels of validation data\n",
    "(Y_validation,Y_validation_orig)= OneHotEncoding(Y_validation,num_validation)\n",
    "\n",
    "\n",
    "print(X_train.shape,Y_train.shape, X_test.shape, Y_test.shape, X_validation.shape, Y_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474,
     "referenced_widgets": [
      "793b49b005ba4163b3b29f1b93532822",
      "45db035662e447f4ace3dbcb1135b058",
      "ab235de3cbb64b8bb36e19998c5b12c8",
      "ec347fac27a94115ad2465f6bc95876a",
      "cee2374be7044934b0fe158c8215c3e9",
      "dc532b8236bb4eed8b20dd5966b260ac",
      "498708455d3c4c82bf9b842c99e4016e",
      "ed81acdae1854eb78aa02650a60d49af"
     ]
    },
    "id": "OlBuaELrjM_F",
    "outputId": "0ea4f296-36ab-4f9c-f73b-247bf6794107"
   },
   "outputs": [],
   "source": [
    "# calling wandb agent to perform hyperparameter sweep\n",
    "wandb.agent(sweep_id, TrainModel,count = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7QFV8rEO4ywb"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AvvVS1PdplzT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "final_final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "45db035662e447f4ace3dbcb1135b058": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "498708455d3c4c82bf9b842c99e4016e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "793b49b005ba4163b3b29f1b93532822": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ab235de3cbb64b8bb36e19998c5b12c8",
       "IPY_MODEL_ec347fac27a94115ad2465f6bc95876a"
      ],
      "layout": "IPY_MODEL_45db035662e447f4ace3dbcb1135b058"
     }
    },
    "ab235de3cbb64b8bb36e19998c5b12c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc532b8236bb4eed8b20dd5966b260ac",
      "placeholder": "​",
      "style": "IPY_MODEL_cee2374be7044934b0fe158c8215c3e9",
      "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r"
     }
    },
    "cee2374be7044934b0fe158c8215c3e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dc532b8236bb4eed8b20dd5966b260ac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ec347fac27a94115ad2465f6bc95876a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed81acdae1854eb78aa02650a60d49af",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_498708455d3c4c82bf9b842c99e4016e",
      "value": 1
     }
    },
    "ed81acdae1854eb78aa02650a60d49af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
