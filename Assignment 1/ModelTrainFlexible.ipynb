{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "G5gA0Y4f8NOG"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist as data\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hsi1qyMmwwJj",
    "outputId": "cbbe0f31-a52b-4407-c085-8cec713f083d"
   },
   "outputs": [],
   "source": [
    "# !pip install wandb\n",
    "# import wandb\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bNwTSrbL1NL4"
   },
   "outputs": [],
   "source": [
    "# Flattening input training data each image of size 28x28 pixels into a vector of size 784\n",
    "def FlattenInput(X_train, X_test):\n",
    "  num_train = X_train.shape[0]\n",
    "  num_test = X_test.shape[0]\n",
    "  \n",
    "  features = X_train.shape[1]*X_train.shape[2]  # 28x28 = 784\n",
    "  X_train=X_train.reshape(num_train, features)\n",
    "  X_test=X_test.reshape(num_test,features)\n",
    "  \n",
    "\n",
    "  X_train=np.transpose(X_train)\n",
    "  X_test = np.transpose(X_test)\n",
    "  \n",
    "\n",
    "  X_train = X_train/255  # normalised data\n",
    "  X_test = X_test/255\n",
    " \n",
    "\n",
    "  return(X_train, X_test)\n",
    "\n",
    "# One hot encoding of output labels of training data\n",
    "def OneHotEncoding(Y_train,num_train):\n",
    "  Y_train_orig= Y_train[:,:]\n",
    "\n",
    "  Y_train=np.zeros([10,num_train])\n",
    "\n",
    "  for i in range(num_train):\n",
    "    index=Y_train_orig[0,i]\n",
    "    Y_train[index,i]=1\n",
    "\n",
    "  return(Y_train, Y_train_orig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3kcmg2eahorl"
   },
   "outputs": [],
   "source": [
    "# Shuffles  training data\n",
    "def ShuffleData(X_train, Y_train):\n",
    "  m=X_train.shape[1]\n",
    "  permutation = list(np.random.permutation(m))\n",
    "  X_train = X_train[:, permutation]\n",
    "  Y_train = Y_train[:, permutation]\n",
    "  return(X_train,Y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Divides input training data into mini batches of given batch size\n",
    "def GetMiniBatches(X_train, Y_train,mini_batch_size):\n",
    "    m=X_train.shape[1]  \n",
    "    num_batches = m//mini_batch_size\n",
    "    X_mini_batches = []\n",
    "    Y_mini_batches = []\n",
    "\n",
    "    #X_train, Y_train = ShuffleData(X_train, Y_train)\n",
    "\n",
    "    for i in range(num_batches):\n",
    "      x=X_train[:,i*mini_batch_size:(i+1)*(mini_batch_size)]\n",
    "      y=Y_train[:,i*mini_batch_size:(i+1)*(mini_batch_size)]\n",
    "      X_mini_batches.append(x)\n",
    "      Y_mini_batches.append(y)\n",
    "\n",
    "    if m%mini_batch_size!=0:\n",
    "      index = num_batches*mini_batch_size\n",
    "      x=X_train[:,index:index+m%mini_batch_size]\n",
    "      y=Y_train[:,index:index+m%mini_batch_size]\n",
    "      X_mini_batches.append(x)\n",
    "      Y_mini_batches.append(y)\n",
    "    \n",
    "    \n",
    "    return(X_mini_batches, Y_mini_batches)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "d5qjwu7m0NTt"
   },
   "outputs": [],
   "source": [
    "# Activations and their derivatives\n",
    "\n",
    "\n",
    "# Computes Relu activation function\n",
    "def Relu(Z):\n",
    "\n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    return A\n",
    "\n",
    "# Computes derivative of Relu Activation\n",
    "def ReluGradient(z):\n",
    "    dZ = np.zeros(z.shape) \n",
    "    dZ[z > 0] = 1\n",
    "    assert (dZ.shape == z.shape)\n",
    "    return dZ\n",
    "\n",
    "\n",
    "# Computes Sigmoid activation function\n",
    "def Sigmoid(z):\n",
    "    a=1/(1+np.exp(-z))\n",
    "    return(a)\n",
    "\n",
    "\n",
    "# Computes derivative of Sigmoid Activation\n",
    "def SigmoidGradient(z):\n",
    "    a=Sigmoid(z)\n",
    "    return(a*(1-a))\n",
    "\n",
    "# Computes Tanh activation function\n",
    "def Tanh(z):\n",
    "  a=np.tanh(z)\n",
    "  return(a)\n",
    "\n",
    "# Computes derivative of Tanh Activation\n",
    "def TanhGradient(z):\n",
    "  a=Tanh(z)\n",
    "  return(1-a**2)\n",
    "\n",
    "#  Computes Softmax activation function\n",
    "def Softmax(z):\n",
    "    num=np.exp(z)\n",
    "    den=np.sum(np.exp(z),axis=0)\n",
    "    a=num/den\n",
    "    return(a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Rzk5ZpZqBEir"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Initialize weights using Xavier or Random weight initialisations\n",
    "def InitializeWeights(layers, params, M, R, initialiser):\n",
    "  seed=3\n",
    "  np.random.seed(seed)\n",
    "  for i in range(1,len(layers)):\n",
    "      if(initialiser==\"xavier\"):\n",
    "        params[\"W\"+str(i)]=np.random.randn(layers[i],layers[i-1])*np.sqrt(2 / (layers[i]+layers[i-1]))\n",
    "      elif(initialiser==\"random\"):\n",
    "        params[\"W\"+str(i)]=np.random.normal(2,1,[layers[i],layers[i-1]])\n",
    "        \n",
    "\n",
    "      params[\"b\"+str(i)]=np.zeros([layers[i],1])\n",
    "      M[\"W\"+str(i)]=np.zeros([layers[i],layers[i-1]])\n",
    "      M[\"b\"+str(i)]=np.zeros([layers[i],1])\n",
    "      R[\"W\"+str(i)]=np.zeros([layers[i],layers[i-1]])\n",
    "      R[\"b\"+str(i)]=np.zeros([layers[i],1])\n",
    "  return(params, M, R)\n",
    "\n",
    "\n",
    "# computes loss through cross entropy error function\n",
    "def CrossEntropyError(a,Y,params,layers,weight_decay):\n",
    "    m=a.shape[1]\n",
    "    error=-(np.sum(np.sum(Y*np.log(a),axis=1),axis=0))\n",
    "    #error=-np.sum(Y*np.log(a)+(1-Y)*np.log(1-a))/m\n",
    "    for i in range(1,len(layers)):\n",
    "      regu_cost = (weight_decay/2)*np.sum(np.sum(np.square(params[\"W\"+str(i)]),axis=1),axis=0)\n",
    "    error+=regu_cost\n",
    "    return(error)\n",
    "\n",
    "\n",
    "# computes loss through Mean Squared Error Function   \n",
    "def MeanSquaredError(a,Y,params,layers,weight_decay):\n",
    "    m=a.shape[1]\n",
    "    error= np.sum(np.sum(np.square(a-Y),axis=1),axis=0)/2\n",
    "    for i in range(1,len(layers)):\n",
    "      regu_cost = (weight_decay/2)*np.sum(np.sum(np.square(params[\"W\"+str(i)]),axis=1),axis=0)\n",
    "    error+=regu_cost\n",
    "    return(error)\n",
    "\n",
    "\n",
    "# Computes pre-activations and activations through forward propagation\n",
    "def ForwardPropagation(params,layers,X,activation):\n",
    "    \n",
    "    a=X\n",
    "    n=len(layers)-1\n",
    "    Z=[]\n",
    "    A=[]\n",
    "    A.append(X)\n",
    "    Z.append(X)\n",
    "    for i in range(1,len(layers)-1):\n",
    "        W=params[\"W\"+str(i)]\n",
    "        b=params[\"b\"+str(i)]\n",
    "        z = np.dot(W,a)+b\n",
    "        if(activation==\"relu\"):\n",
    "          a = Relu(z)\n",
    "        elif(activation==\"sigmoid\"):\n",
    "          a = Sigmoid(z)\n",
    "        elif(activation==\"tanh\"):\n",
    "          a = Tanh(z)\n",
    "        A.append(a)\n",
    "        Z.append(z)\n",
    "    W=params[\"W\"+str(n)]\n",
    "    b=params[\"b\"+str(n)]\n",
    "    z = np.dot(W,a)+b\n",
    "    a = Softmax(z)\n",
    "    Z.append(z)\n",
    "    A.append(a)\n",
    "    return(A,Z)\n",
    "\n",
    "\n",
    "# Computes gradients of weights through backward propagation\n",
    "def BackwardPropagation(params, layers, Z, A, learning_rate, Y, activation, loss_function):\n",
    "    m=Y.shape[1]\n",
    "    l=len(layers)-1\n",
    "    if(loss_function=='cross-entropy'):\n",
    "        dz= A[l]-Y\n",
    "    elif(loss_function=='MSE'):\n",
    "        dz= np.multiply((A[l]- Y), np.multiply(A[l], (1 - A[l])))\n",
    "      \n",
    "    gradients={}\n",
    "    while(l>=0):\n",
    "        dw = np.dot(dz,np.transpose(A[l-1]))/m\n",
    "        db = np.sum(dz,axis=1)/m \n",
    "        db=db.reshape(db.shape[0],1)\n",
    "        gradients[\"dw\"+str(l)]=dw\n",
    "        gradients[\"db\"+str(l)]=db\n",
    "        if(l>=2):\n",
    "            da= np.dot(np.transpose(params[\"W\"+str(l)]), dz)\n",
    "            if(activation==\"relu\"):\n",
    "              dz = da*ReluGradient(Z[l-1])\n",
    "            elif(activation==\"sigmoid\"):\n",
    "              dz = da*SigmoidGradient(Z[l-1])\n",
    "            elif(activation==\"tanh\"):\n",
    "              dz = da*TanhGradient(Z[l-1])\n",
    "        l=l-1\n",
    "    return(gradients)\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "y9EruJ95G0-E"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Updates weights through Stochastic Gradient Descent\n",
    "def UpdateWeightsSGD(m, params,gradients,layers, learning_rate, weight_decay):\n",
    "    for i in range(1,len(layers)):\n",
    "        params[\"W\"+str(i)]-= learning_rate*gradients[\"dw\"+str(i)]\n",
    "        params[\"b\"+str(i)]-= learning_rate*gradients[\"db\"+str(i)]\n",
    "        params[\"W\"+str(i)]-= learning_rate*(weight_decay/m)*params[\"W\"+str(i)]\n",
    "        params[\"b\"+str(i)]-= learning_rate*(weight_decay/m)*params[\"b\"+str(i)]\n",
    "    return(params)\n",
    "\n",
    "\n",
    "# Updates weights through Momentum Gradient Descent\n",
    "def UpdateWeightsMomentum(m, params, gradients, M, layers, learning_rate,weight_decay, beta1):\n",
    "    for i in range(1,len(layers)):\n",
    "        M[\"W\"+str(i)]=beta1*M[\"W\"+str(i)]+(1-beta1)*gradients[\"dw\"+str(i)]\n",
    "        M[\"b\"+str(i)]=beta1*M[\"b\"+str(i)]+(1-beta1)*gradients[\"db\"+str(i)]\n",
    "        params[\"W\"+str(i)]-= learning_rate* M[\"W\"+str(i)]\n",
    "        params[\"b\"+str(i)]-= learning_rate*M[\"b\"+str(i)]\n",
    "        params[\"W\"+str(i)]-= learning_rate*(weight_decay/m)*params[\"W\"+str(i)]\n",
    "        params[\"b\"+str(i)]-= learning_rate*(weight_decay/m)*params[\"b\"+str(i)]\n",
    "    return(params,M)\n",
    "\n",
    "\n",
    "# Updates weights through RMSprop Gradient Descent\n",
    "def UpdateWeightsRMS(m, params, gradients,R, layers, learning_rate, weight_decay, beta1 ,eps):\n",
    "\n",
    "    for i in range(1,len(layers)):\n",
    "        R[\"W\"+str(i)]=beta1*R[\"W\"+str(i)]+(1-beta1)*np.power(gradients[\"dw\"+str(i)],2)\n",
    "        R[\"b\"+str(i)]=beta1*R[\"b\"+str(i)]+(1-beta1)*np.power(gradients[\"db\"+str(i)],2)\n",
    "        params[\"W\"+str(i)]-= (learning_rate*gradients[\"dw\"+str(i)])/(np.sqrt(R[\"W\"+str(i)])+eps)\n",
    "        params[\"b\"+str(i)]-= (learning_rate*gradients[\"db\"+str(i)])/(np.sqrt(R[\"b\"+str(i)])+eps)\n",
    "        params[\"W\"+str(i)]-= learning_rate*(weight_decay/m)*params[\"W\"+str(i)]\n",
    "        params[\"b\"+str(i)]-= learning_rate*(weight_decay/m)*params[\"b\"+str(i)]\n",
    "    return(params,R)\n",
    "\n",
    "\n",
    "# Updates weights through Nesterov Accelerated Gradient Descent\n",
    "def UpdateWeightsNesterov(m, params, lookahead_grads, M, layers, learning_rate, weight_decay, beta1):\n",
    "    \n",
    "    for i in range(1,len(layers)):\n",
    "        M[\"W\"+str(i)]=beta1*M[\"W\"+str(i)]+(1-beta1)*lookahead_grads[\"dw\"+str(i)]\n",
    "        M[\"b\"+str(i)]=beta1*M[\"b\"+str(i)]+(1-beta1)*lookahead_grads[\"db\"+str(i)]\n",
    "        params[\"W\"+str(i)]-= learning_rate* M[\"W\"+str(i)]\n",
    "        params[\"b\"+str(i)]-= learning_rate*M[\"b\"+str(i)]\n",
    "        params[\"W\"+str(i)]-= learning_rate*(weight_decay/m)*params[\"W\"+str(i)]\n",
    "        params[\"b\"+str(i)]-= learning_rate*(weight_decay/m)*params[\"b\"+str(i)]\n",
    "    return(params,M)\n",
    "\n",
    "# Updates weights through Nesterov Adam Gradient Descent\n",
    "def UpdateWeightsNAdam(m, params, lookahead_grads, layers, M, R, learning_rate, weight_decay, beta1, beta2, eps, t):\n",
    "    M_c={}\n",
    "    R_c={}\n",
    "    for i in range(1,len(layers)):\n",
    "        M[\"W\"+str(i)]=beta1*M[\"W\"+str(i)]+(1-beta1)*lookahead_grads[\"dw\"+str(i)]\n",
    "        M[\"b\"+str(i)]=beta1*M[\"b\"+str(i)]+(1-beta1)*lookahead_grads[\"db\"+str(i)]\n",
    "\n",
    "        M_c[\"W\"+str(i)]=M[\"W\"+str(i)]/(1-np.power(beta1,t))   # bias correction\n",
    "        M_c[\"b\"+str(i)]=M[\"b\"+str(i)]/(1-np.power(beta1,t))\n",
    "        R[\"W\"+str(i)]=beta2*R[\"W\"+str(i)]+(1-beta2)*np.power(lookahead_grads[\"dw\"+str(i)],2)\n",
    "        R[\"b\"+str(i)]=beta2*R[\"b\"+str(i)]+(1-beta2)*np.power(lookahead_grads[\"db\"+str(i)],2)\n",
    "\n",
    "        R_c[\"W\"+str(i)]=R[\"W\"+str(i)]/(1-np.power(beta2,t))     # bias correction\n",
    "        R_c[\"b\"+str(i)]=R[\"b\"+str(i)]/(1-np.power(beta2,t))\n",
    "        params[\"W\"+str(i)]-= (learning_rate*M_c[\"W\"+str(i)])/(np.sqrt(R_c[\"W\"+str(i)])+eps)\n",
    "        params[\"b\"+str(i)]-= (learning_rate*M_c[\"b\"+str(i)])/(np.sqrt(R_c[\"b\"+str(i)])+eps)\n",
    "        params[\"W\"+str(i)]-= learning_rate*(weight_decay/m)*params[\"W\"+str(i)]\n",
    "        params[\"b\"+str(i)]-= learning_rate*(weight_decay/m)*params[\"b\"+str(i)]\n",
    "        \n",
    "    return(params,M,R)\n",
    "\n",
    "# Updates weights through Adam Gradient Descent\n",
    "def UpdateWeightsAdam(m, params, gradients, layers, M, R, learning_rate, weight_decay, beta1, beta2, eps, t):\n",
    "    M_c={}\n",
    "    R_c={}\n",
    "    for i in range(1,len(layers)):\n",
    "        M[\"W\"+str(i)]=beta1*M[\"W\"+str(i)]+(1-beta1)*gradients[\"dw\"+str(i)]\n",
    "        M[\"b\"+str(i)]=beta1*M[\"b\"+str(i)]+(1-beta1)*gradients[\"db\"+str(i)]\n",
    "\n",
    "        M_c[\"W\"+str(i)]=M[\"W\"+str(i)]/(1-np.power(beta1,t))   # bias correction\n",
    "        M_c[\"b\"+str(i)]=M[\"b\"+str(i)]/(1-np.power(beta1,t))\n",
    "        R[\"W\"+str(i)]=beta2*R[\"W\"+str(i)]+(1-beta2)*np.power(gradients[\"dw\"+str(i)],2)\n",
    "        R[\"b\"+str(i)]=beta2*R[\"b\"+str(i)]+(1-beta2)*np.power(gradients[\"db\"+str(i)],2)\n",
    "\n",
    "        R_c[\"W\"+str(i)]=R[\"W\"+str(i)]/(1-np.power(beta2,t))    # bias correction\n",
    "        R_c[\"b\"+str(i)]=R[\"b\"+str(i)]/(1-np.power(beta2,t))\n",
    "        params[\"W\"+str(i)]-= (learning_rate*M_c[\"W\"+str(i)])/(np.sqrt(R_c[\"W\"+str(i)])+eps)\n",
    "        params[\"b\"+str(i)]-= (learning_rate*M_c[\"b\"+str(i)])/(np.sqrt(R_c[\"b\"+str(i)])+eps)\n",
    "        params[\"W\"+str(i)]-= learning_rate*(weight_decay/m)*params[\"W\"+str(i)]\n",
    "        params[\"b\"+str(i)]-= learning_rate*(weight_decay/m)*params[\"b\"+str(i)]\n",
    "    return(params,M,R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Sms3y9Ma0LoE"
   },
   "outputs": [],
   "source": [
    "# predicts training data accuracy, validation data accuracy, test data accuracy of given model\n",
    "def Predict(params, layers, X_train, X_test, Y_train_orig, Y_test,activation,weight_decay):\n",
    "  m=X_validation.shape[1]\n",
    "  A,Z=ForwardPropagation(params, layers, X_validation,activation)\n",
    "  val_error = CrossEntropyError(A[-1], Y_validation,params,layers,weight_decay)\n",
    "  val_error = val_error/m\n",
    "\n",
    "  num_train = X_train.shape[1]\n",
    "  num_test = X_test.shape[1]\n",
    "  num_validation = X_validation.shape[1]\n",
    "  \n",
    "  #predicts training data accuracy\n",
    "  A,Z=ForwardPropagation(params, layers, X_train, activation)\n",
    "  pred=A[-1]\n",
    "  max_index = np.argmax(pred, axis=0)\n",
    "  count=0\n",
    "  for i in range(num_train):\n",
    "      if(Y_train_orig[0,i]==max_index[i]):\n",
    "          count+=1\n",
    "  train_accuracy = (count/num_train)*100\n",
    "\n",
    "  #predicts validation data accuracy\n",
    "  A,Z=ForwardPropagation(params, layers, X_validation, activation)\n",
    "  pred=A[-1]\n",
    "  max_index = np.argmax(pred, axis=0)\n",
    "  count=0\n",
    "  for i in range(num_validation):\n",
    "      if(Y_validation_orig[0,i]==max_index[i]):\n",
    "          count+=1\n",
    "\n",
    "  Y_pred=max_index\n",
    "\n",
    "  \n",
    "\n",
    "  val_accuracy = (count/num_validation)*100\n",
    "\n",
    "  #predicts testing data accuracy\n",
    "  A,Z=ForwardPropagation(params, layers, X_test,activation)\n",
    "  pred=A[-1]\n",
    "  max_index = np.argmax(pred, axis=0)\n",
    "  count=0\n",
    "  for i in range(num_test):\n",
    "      if(Y_test[0,i]==max_index[i]):\n",
    "          count+=1\n",
    "\n",
    "  test_accuracy = (count/num_test)*100\n",
    "\n",
    "  return(val_error,train_accuracy,val_accuracy,test_accuracy,Y_pred)\n",
    "\n",
    "  \n",
    "          \n",
    "      \n",
    "# creates confusion matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zsaDS2Ala_k9",
    "outputId": "62f675c3-abac-4bee-be6c-21f302bd8108"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-91nnO5y-yUr",
    "outputId": "ffb93404-355f-4393-b7b4-fe0a2be7a9b1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "yAuAvnAdi4Hm"
   },
   "outputs": [],
   "source": [
    "# Main Component for training our model\n",
    "\n",
    "def TrainModel():\n",
    "    \n",
    "    # flexible architecture, taking user input\n",
    "    layers=[X_train.shape[0]]\n",
    "    hidden_layers= int(input(\"Enter number of hidden layers\"))       # eg: 3,4,5\n",
    "    for i in range(0,hidden_layers):\n",
    "        layers.append(int(input(\"enter the number of nodes in \"+str(i+1)+\" hidden layer.\")))\n",
    "    layers.append(Y_train.shape[0])\n",
    "    \n",
    "    \n",
    "    learning_rate=0.001    # eg: 0.001, 0.0001\n",
    "    weight_decay=0          # eg:  0, 0.0005, 0.5\n",
    "    optimiser=\"adam\"       # eg: \"sgd\", \"momentum\", \"nesterov\", \"rmsprop\", \"adam\", \"nadam\"\n",
    "    activation=\"relu\"       # eg: 'sigmoid', 'tanh','relu'\n",
    "    epochs=10               # eg: 5,10\n",
    "    mini_batch_size=16      # eg: 16, 32, 64\n",
    "    initialiser='xavier'    # eg: \"random\", \"xavier\"\n",
    "\n",
    "    \n",
    "    loss_function='cross-entropy'\n",
    "    \n",
    "    training_errors=[]\n",
    "    validation_errors=[]\n",
    "    \n",
    "    params={}\n",
    "    M={}\n",
    "    R={}\n",
    "    lookahead_params={}\n",
    "\n",
    "    #wandb.run.name = \"hl_\" + str(wandb.config.hidden_layers) +\"_hn_\" + str(wandb.config.size_hidden_layer)  + \"_opt_\" + wandb.config.optimiser +\"_lr_\" + str(wandb.config.learning_rate)+ \"_init_\" + wandb.config.initialiser +\"_bs_\"+str(wandb.config.batch_size)+\"_ac_\" + wandb.config.activation+ \"_wd_\"+str(weight_decay)\n",
    "\n",
    "    \n",
    "    # initialise weights\n",
    "    params, M, R = InitializeWeights(layers, params, M, R,initialiser)\n",
    "    (X_mini_batches, Y_mini_batches) = GetMiniBatches(X_train, Y_train,mini_batch_size)\n",
    "\n",
    "    epoch=1\n",
    "    beta1=0.9  # hyperparameter for exponentially weighted averages of momentum\n",
    "    beta2=0.999  # hyperparameter for exponentially weighted average of  rms\n",
    "    eps=1e-8     # small real value so that denominator do not become zero\n",
    "   \n",
    "    m=X_train.shape[1]\n",
    "    \n",
    "    t=0\n",
    "\n",
    "    while(epoch<=epochs):\n",
    "        train_error=0\n",
    "        for i in range(len(X_mini_batches)):\n",
    "          X_train_mini=X_mini_batches[i]\n",
    "          Y_train_mini=Y_mini_batches[i]\n",
    "\n",
    "          # Forward Propagation\n",
    "          A,Z=ForwardPropagation(params, layers, X_train_mini,activation)\n",
    "\n",
    "          # Computes Loss function\n",
    "          if(loss_function == \"cross-entropy\"):\n",
    "            train_error += CrossEntropyError(A[-1], Y_train_mini,params,layers,weight_decay)\n",
    "          elif(loss_function == \"MSE\"):\n",
    "            train_error += MeanSquaredError(A[-1],Y_train_mini,params,layers,weight_decay)\n",
    "          \n",
    "          \n",
    "          if(optimiser==\"nadam\" or optimiser==\"nesterov\"):\n",
    "\n",
    "              # Initialising lookaheads for nadam and nesterov\n",
    "              for i in range(1,len(layers)):\n",
    "                lookahead_params[\"W\"+str(i)]=params[\"W\"+str(i)]-M[\"W\"+str(i)]\n",
    "                lookahead_params[\"b\"+str(i)]=params[\"b\"+str(i)]-M[\"b\"+str(i)]\n",
    "\n",
    "              # Back propagation for nada and nesterov\n",
    "              lookahead_grads=BackwardPropagation(lookahead_params, layers,Z, A, learning_rate, Y_train_mini,activation,loss_function)\n",
    "\n",
    "              # Updating weights\n",
    "              if(optimiser==\"nesterov\"):\n",
    "                params, M = UpdateWeightsNesterov(num_train, params, lookahead_grads, M, layers, learning_rate, weight_decay, beta1)\n",
    "              elif(optimiser==\"nadam\"):\n",
    "                t=t+1\n",
    "                params,M,R = UpdateWeightsNAdam(num_train, params, lookahead_grads, layers, M, R, learning_rate, weight_decay, beta1,beta2,eps,t)\n",
    "\n",
    "          else:\n",
    "              # Back propagation sgd, momentum, rmsprop , adam\n",
    "              gradients=BackwardPropagation(params, layers, Z , A, learning_rate, Y_train_mini, activation, loss_function)\n",
    "\n",
    "              # updating weights\n",
    "              if(optimiser==\"sgd\"):\n",
    "                params = UpdateWeightsSGD(num_train, params,gradients,layers, learning_rate, weight_decay)\n",
    "              elif(optimiser==\"momentum\"):\n",
    "                params,M = UpdateWeightsMomentum(num_train, params, gradients, M, layers, learning_rate,weight_decay, beta1)\n",
    "              elif(optimiser==\"rmsprop\"):\n",
    "                params, R = UpdateWeightsRMS(num_train, params, gradients, R, layers, learning_rate, weight_decay, beta1 ,eps)\n",
    "              elif(optimiser==\"adam\"):\n",
    "                t=t+1\n",
    "                params, M, R = UpdateWeightsAdam(num_train,params, gradients, layers, M, R, learning_rate, weight_decay, beta1, beta2, eps, t)\n",
    "\n",
    "\n",
    "        # average training error\n",
    "        train_error=train_error/m\n",
    "        training_errors.append(train_error)\n",
    "\n",
    "        # predicts training data accuracy, validation data accuracy, test data accuracy of given model\n",
    "        val_error, train_accuracy, val_accuracy, test_accuracy, Y_pred = Predict(params, layers, X_train, X_test, Y_train_orig, Y_test,activation, weight_decay)\n",
    "        validation_errors.append(val_error)\n",
    "        \n",
    "        epoch+=1\n",
    "        print(\"epoch: \"+str(10), \"training error: \"+str(train_error), \"training accuracy: \"+str(train_accuracy), \"validation error: \"+str(val_error), \"validation accuracy: \"+str(val_accuracy))\n",
    "        \n",
    "    #predicts training data accuracy, validation data accuracy, test data accuracy of given model\n",
    "    val_error, train_accuracy, val_accuracy, test_accuracy, Y_pred = Predict(params, layers, X_train, X_test, Y_train_orig, Y_test,activation, weight_decay)\n",
    "\n",
    "    \n",
    "    plt.plot(training_errors)\n",
    "    plt.plot(validation_errors)\n",
    "    plt.xlabel(\"EPOCHS\")\n",
    "    plt.ylabel(\"Loss value\")\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "u0TgTG9ZM9CI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 54000) (10, 54000) (784, 10000) (1, 10000) (784, 6000) (10, 6000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loading Fashion MNIST dataset\n",
    "(X, Y), (X_test, Y_test) = data.load_data()\n",
    "\n",
    "# Flattening of Input data\n",
    "(X, X_test) = FlattenInput(X, X_test)\n",
    "\n",
    "Y = Y.reshape(1, X.shape[1])\n",
    "Y_test= Y_test.reshape(1, X_test.shape[1])\n",
    "\n",
    "\n",
    "# Shuffling of training data\n",
    "X,Y = ShuffleData(X, Y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Splitting input training data into training data and validation data in the ratio of 9:1\n",
    "num_train= int(0.9*X.shape[1])\n",
    "num_validation = int(0.1*X.shape[1])\n",
    "num_test=X_test.shape[1]\n",
    "\n",
    "\n",
    "\n",
    "X_train = X[:,:num_train]\n",
    "X_validation = X[:,num_train:]\n",
    "\n",
    "Y_train = Y[:,:num_train]\n",
    "Y_validation =  Y[:,num_train:]\n",
    "\n",
    "\n",
    "# One Hot Encoding of output labels of training data\n",
    "(Y_train,Y_train_orig)= OneHotEncoding(Y_train,num_train)\n",
    "\n",
    "# One Hot Encoding of output labels of validation data\n",
    "(Y_validation,Y_validation_orig)= OneHotEncoding(Y_validation,num_validation)\n",
    "\n",
    "\n",
    "print(X_train.shape,Y_train.shape, X_test.shape, Y_test.shape, X_validation.shape, Y_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474,
     "referenced_widgets": [
      "793b49b005ba4163b3b29f1b93532822",
      "45db035662e447f4ace3dbcb1135b058",
      "ab235de3cbb64b8bb36e19998c5b12c8",
      "ec347fac27a94115ad2465f6bc95876a",
      "cee2374be7044934b0fe158c8215c3e9",
      "dc532b8236bb4eed8b20dd5966b260ac",
      "498708455d3c4c82bf9b842c99e4016e",
      "ed81acdae1854eb78aa02650a60d49af"
     ]
    },
    "id": "OlBuaELrjM_F",
    "outputId": "0ea4f296-36ab-4f9c-f73b-247bf6794107"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter number of hidden layers3\n",
      "enter the number of nodes in 1 hidden layer.64\n",
      "enter the number of nodes in 2 hidden layer.64\n",
      "enter the number of nodes in 3 hidden layer.64\n",
      "epoch: 10 training error: 0.5083525013769101 training accuracy: 85.23148148148148 validation error: 0.443779202714026 validation accuracy: 83.0\n",
      "epoch: 10 training error: 0.3804931521412127 training accuracy: 87.5 validation error: 0.3899476471076104 validation accuracy: 85.43333333333332\n",
      "epoch: 10 training error: 0.3435784598208189 training accuracy: 87.92777777777778 validation error: 0.37808779348448424 validation accuracy: 85.88333333333334\n",
      "epoch: 10 training error: 0.3204213495115603 training accuracy: 87.97592592592592 validation error: 0.38732619323349377 validation accuracy: 85.86666666666667\n",
      "epoch: 10 training error: 0.30337613972202615 training accuracy: 89.46111111111111 validation error: 0.34884952865093427 validation accuracy: 87.11666666666666\n",
      "epoch: 10 training error: 0.28948596428214535 training accuracy: 89.0425925925926 validation error: 0.37447667245307525 validation accuracy: 86.56666666666666\n",
      "epoch: 10 training error: 0.2782465659751432 training accuracy: 89.76481481481483 validation error: 0.3584135380407156 validation accuracy: 87.28333333333333\n",
      "epoch: 10 training error: 0.26902071474115485 training accuracy: 89.71481481481482 validation error: 0.3692250530865705 validation accuracy: 87.28333333333333\n",
      "epoch: 10 training error: 0.2594982866565584 training accuracy: 90.09814814814816 validation error: 0.35801814517823105 validation accuracy: 87.41666666666667\n",
      "epoch: 10 training error: 0.25250542027166706 training accuracy: 90.54444444444445 validation error: 0.3585049310891422 validation accuracy: 87.88333333333334\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# calling wandb agent to perform hyperparameter sweep\n",
    "# wandb.agent(sweep_id, TrainModel,count = 1)\n",
    "TrainModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7QFV8rEO4ywb"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AvvVS1PdplzT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "final_final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "45db035662e447f4ace3dbcb1135b058": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "498708455d3c4c82bf9b842c99e4016e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "793b49b005ba4163b3b29f1b93532822": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ab235de3cbb64b8bb36e19998c5b12c8",
       "IPY_MODEL_ec347fac27a94115ad2465f6bc95876a"
      ],
      "layout": "IPY_MODEL_45db035662e447f4ace3dbcb1135b058"
     }
    },
    "ab235de3cbb64b8bb36e19998c5b12c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc532b8236bb4eed8b20dd5966b260ac",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_cee2374be7044934b0fe158c8215c3e9",
      "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r"
     }
    },
    "cee2374be7044934b0fe158c8215c3e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dc532b8236bb4eed8b20dd5966b260ac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ec347fac27a94115ad2465f6bc95876a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed81acdae1854eb78aa02650a60d49af",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_498708455d3c4c82bf9b842c99e4016e",
      "value": 1
     }
    },
    "ed81acdae1854eb78aa02650a60d49af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
