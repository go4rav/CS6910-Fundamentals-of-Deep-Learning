{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ActcqAhENlxN"
      },
      "outputs": [],
      "source": [
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "from keras import layers\n",
        "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
        "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCPOkmN1NlxR"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "train_dir='../input/inaturalist/inaturalist_12K/train'\n",
        "test_dir='../input/inaturalist/inaturalist_12K/val'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxMuAR2oNlxS"
      },
      "outputs": [],
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QhVLVArNlxT"
      },
      "outputs": [],
      "source": [
        "# configure sweep parameters\n",
        "sweep_config = {\n",
        "  \"name\": \"Bayesian Sweep\",\n",
        "  \"method\": \"bayes\",\n",
        "  \"metric\":{\n",
        "  \"name\": \"val_accuracy\",\n",
        "  \"goal\": \"maximize\"\n",
        "  },\n",
        "  \"parameters\": {\n",
        "                   'base_filters': {'values': [32, 64]},\n",
        "                   'filter_org': {'values': [0.5, 1, 2]}, \n",
        "                   'data_augment': {'values': [False, True]},\n",
        "                   'batch_norm': {'values': [False, True]}, \n",
        "                   \"batch_size\": { \"values\": [32, 64] },\n",
        "                   'pool_size' : {'values': [2]},\n",
        "                   'dropout_rate': {'values': [0.0, 0.2, 0.3]},\n",
        "                    'learning_rate':{ 'values': [0.0004 ]},\n",
        "                   'kernel_size': {'values': [3, 5, 7]},\n",
        "                   'dense_size': {'values': [64,512,1024]},\n",
        "                   'activation': {'values': ['relu']},\n",
        "                   'epochs': {'values': [10,20,30]}, \n",
        "                   'conv_layers': {'values': [5]}\n",
        "                }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config,project='CS6910 Assignment2', entity='go4rav')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2JOFJzNDNlxW"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "\n",
        "    # initialising wandb\n",
        "    wandb.init()\n",
        "\n",
        "    # loading configured parameters\n",
        "    CONFIG=wandb.config\n",
        "    base_filters = CONFIG.base_filters\n",
        "    filter_org= CONFIG.filter_org\n",
        "    dropout_rate = CONFIG.dropout_rate\n",
        "    kernel_size = CONFIG.kernel_size\n",
        "    pool_size = CONFIG.pool_size\n",
        "    dense_size = CONFIG.dense_size\n",
        "    data_augment = CONFIG.data_augment\n",
        "    learning_rate = CONFIG.learning_rate\n",
        "    batch_norm = CONFIG.batch_norm\n",
        "    batch_size = CONFIG.batch_size\n",
        "    epochs = CONFIG.epochs\n",
        "    activation = CONFIG.activation\n",
        "    conv_layers = CONFIG.conv_layers\n",
        "    input_shape = (256,256,3)\n",
        "\n",
        "\n",
        "\n",
        "    filters = [base_filters]\n",
        "\n",
        "    # if filter organisation is same in each layer\n",
        "    if filter_org == 1 :\n",
        "      for i in range(1, conv_layers) :\n",
        "        filters.append(filters[i - 1])\n",
        "\n",
        "    # if filter organisation is halves after each layer\n",
        "    elif filter_org == 0.5 :\n",
        "      for i in range(1, conv_layers) :\n",
        "        filters.append(filters[i - 1] / 2)\n",
        "    \n",
        "    # # if filter organisation is doubles after each layer\n",
        "    elif filter_org == 2 :\n",
        "      for i in range(1, conv_layers) :\n",
        "        filters.append(filters[i - 1] * 2)\n",
        "\n",
        "    wandb.run.name =  \"_ft_\"+str(base_filters)+\"_fto_\"+str(filter_org)+\"_ksize_\"+str(kernel_size)+\"_dn_\" + str(dense_size)+ \"_dro_\" + str(dropout_rate) + \"_bs_\"+str(batch_size)+\"_da_\"+str(data_augment)+\"_bn_\"+str(batch_norm)\n",
        "\n",
        "\n",
        "\n",
        "    with tf.device('/device:GPU:0'):\n",
        "      tf.keras.backend.clear_session()\n",
        "      model = Sequential() \n",
        "      for i in range(0,conv_layers): \n",
        "\n",
        "          # adding convolution layer\n",
        "          model.add(Conv2D(filters[i], kernel_size= (kernel_size,kernel_size), input_shape=input_shape, activation=activation))\n",
        "          # adding max_pooling layer\n",
        "          model.add(MaxPooling2D(pool_size=(pool_size,pool_size))) \n",
        "          # adding batch normalization\n",
        "          if(batch_norm == True):\n",
        "            model.add(BatchNormalization())\n",
        "\n",
        "\n",
        "      # Flattening\n",
        "      model.add(Flatten()) \n",
        "\n",
        "      # Adding a dense layer\n",
        "      model.add(Dense(dense_size, activation=activation))\n",
        "\n",
        "      # Adding batch normalsation\n",
        "      if(batch_norm == True):\n",
        "            model.add(BatchNormalization())\n",
        "\n",
        "      # adding dropout \n",
        "      model.add(tf.keras.layers.Dropout(rate=dropout_rate))\n",
        "\n",
        "      # adding output layer\n",
        "      model.add(Dense(10, activation='softmax'))\n",
        "      \n",
        "      model.summary()\n",
        "      img_height,img_width=(256,256)\n",
        "\n",
        "\n",
        "      # data augmentation\n",
        "      if data_augment == True:\n",
        "        datagen= ImageDataGenerator(\n",
        "        rotation_range=30,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        rescale=1.0 / 255,\n",
        "        validation_split=0.1,\n",
        "        )\n",
        "      else:\n",
        "        datagen = ImageDataGenerator(\n",
        "        rescale=1.0 / 255,\n",
        "        validation_split=0.1,\n",
        "        )\n",
        "\n",
        "      train_data = datagen.flow_from_directory(\n",
        "      train_dir,\n",
        "      target_size=(img_height, img_width),\n",
        "      batch_size= batch_size,\n",
        "      class_mode='categorical',\n",
        "      shuffle=True,\n",
        "      subset='training',\n",
        "      seed=100,\n",
        "      )\n",
        "\n",
        "      valid_data = datagen.flow_from_directory(\n",
        "      train_dir,\n",
        "      target_size=(img_height, img_width),\n",
        "      class_mode='categorical',\n",
        "      shuffle=True,\n",
        "      subset='validation',\n",
        "      seed=100,\n",
        "      )\n",
        "\n",
        "      optimiser = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "      # model compilation\n",
        "      model.compile(optimizer=optimiser, loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
        "\n",
        "      # model fitting\n",
        "      model.fit(train_data, epochs=epochs, validation_data=valid_data, callbacks=[tf.keras.callbacks.EarlyStopping(monitor = 'val_acc', patience = 5),\n",
        "                                            wandb.keras.WandbCallback()])\n",
        "\n",
        "      # model saving\n",
        "      model.save('./TrainedModel/'+wandb.run.name)\n",
        "      wandb.finish()\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EnIBfCqvNlxX"
      },
      "outputs": [],
      "source": [
        "wandb.agent(sweep_id, train ,count = 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvsEPMTUOynZ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckrVGsg8NlxY"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pV80TofbNlxY"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YUeMVIhAsHdA"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "a1mUP32MqUOi"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tPwzejoytTnt"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WzKmTkP72t4N"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pJXXq5RC3gBO"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jpMbLU8h3xbx"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaS2SY_V4Atf"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vji8oXxqAA5K"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9alhDGSiATZR"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9cTZwTi-GCuT"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vk6zrIJxGIA8"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "part1-kaggle_final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
