{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOdnzNd5C_VY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from keras.layers import Input,Dense,LSTM,GRU,RNN,SimpleRNN,Softmax,Dropout,Concatenate\n",
        "from keras.layers import TimeDistributed\n",
        "import matplotlib.pyplot as plt\n",
        "from attention import AttentionLayer\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87-I61nOlxYq",
        "outputId": "3b74536f-736f-452d-b730-f0af425964dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-15 12:06:07--  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.99.128, 173.194.202.128, 74.125.199.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.99.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2008340480 (1.9G) [application/x-tar]\n",
            "Saving to: ‘dakshina_dataset_v1.0.tar’\n",
            "\n",
            "dakshina_dataset_v1 100%[===================>]   1.87G   225MB/s    in 7.6s    \n",
            "\n",
            "2022-05-15 12:06:15 (250 MB/s) - ‘dakshina_dataset_v1.0.tar’ saved [2008340480/2008340480]\n",
            "\n",
            "dakshina_dataset_v1.0/bn/\n",
            "dakshina_dataset_v1.0/bn/lexicons/\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/romanized/\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/\n",
            "dakshina_dataset_v1.0/gu/lexicons/\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/romanized/\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/\n",
            "dakshina_dataset_v1.0/hi/lexicons/\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/romanized/\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/\n",
            "dakshina_dataset_v1.0/kn/lexicons/\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/kn/romanized/\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/\n",
            "dakshina_dataset_v1.0/ml/lexicons/\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.rom.txt\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/romanized/\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/\n",
            "dakshina_dataset_v1.0/mr/lexicons/\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/romanized/\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/\n",
            "dakshina_dataset_v1.0/pa/lexicons/\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/pa/romanized/\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/\n",
            "dakshina_dataset_v1.0/sd/lexicons/\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/sd/romanized/\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/\n",
            "dakshina_dataset_v1.0/si/lexicons/\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/romanized/\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/\n",
            "dakshina_dataset_v1.0/ta/lexicons/\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/romanized/\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/\n",
            "dakshina_dataset_v1.0/te/lexicons/\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/romanized/\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ur/\n",
            "dakshina_dataset_v1.0/ur/lexicons/\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/romanized/\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/README.md\n"
          ]
        }
      ],
      "source": [
        "!wget https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
        "!tar -xvf '/content/dakshina_dataset_v1.0.tar'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXHYWMh0mwf5"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(file_name):\n",
        "    \n",
        "    input_texts = []\n",
        "    target_texts = []\n",
        "    inputdata=[]\n",
        "    with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.read().split(\"\\n\")\n",
        "    for line in lines[: len(lines) - 1]:\n",
        "        inputdata.append(line)\n",
        "   \n",
        "    for line in inputdata:\n",
        "        target_text,input_text, attestation = line.split(\"\\t\")\n",
        "        \n",
        "        target_text = \"\\t\" + target_text + \"\\n\"\n",
        "        input_texts.append(input_text)\n",
        "        target_texts.append(target_text)\n",
        "        \n",
        "    return(input_texts,target_texts)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1DOiDInwfS0",
        "outputId": "8e16c6d6-87c9-4673-def0-c3e27e953fa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26 65\n",
            "44204 44204\n",
            "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n"
          ]
        }
      ],
      "source": [
        "input_words, target_words = preprocess_data(\"dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\")\n",
        "\n",
        "\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_words])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_words])\n",
        "\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "\n",
        "for input_word in input_words:  \n",
        "  for char in input_word:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "for target_word in target_words:\n",
        "    for char in target_word:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "\n",
        "num_input_tokens = len(input_characters)\n",
        "num_target_tokens = len(target_characters)\n",
        "\n",
        "\n",
        "\n",
        "print(len(input_characters), len(target_characters))\n",
        "input_char_map = dict([(char, i+1) for i, char in enumerate(input_characters)])\n",
        "target_char_map = dict([(char, i+1) for i, char in enumerate(target_characters)])\n",
        "print(len(input_words), len(target_words))\n",
        "print(input_char_map)\n",
        "\n",
        "\n",
        "val_input_words, val_target_words = preprocess_data(\"dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\")\n",
        "\n",
        "test_input_words, test_target_words = preprocess_data(\"dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3D44WtXwkIC",
        "outputId": "34d8a657-5e60-48dc-f11c-57676ef0e6a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(44204, 21, 66)\n",
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n",
            "{1: '\\t', 2: '\\n', 3: 'ँ', 4: 'ं', 5: 'ः', 6: 'अ', 7: 'आ', 8: 'इ', 9: 'ई', 10: 'उ', 11: 'ऊ', 12: 'ऋ', 13: 'ए', 14: 'ऐ', 15: 'ऑ', 16: 'ओ', 17: 'औ', 18: 'क', 19: 'ख', 20: 'ग', 21: 'घ', 22: 'ङ', 23: 'च', 24: 'छ', 25: 'ज', 26: 'झ', 27: 'ञ', 28: 'ट', 29: 'ठ', 30: 'ड', 31: 'ढ', 32: 'ण', 33: 'त', 34: 'थ', 35: 'द', 36: 'ध', 37: 'न', 38: 'प', 39: 'फ', 40: 'ब', 41: 'भ', 42: 'म', 43: 'य', 44: 'र', 45: 'ल', 46: 'व', 47: 'श', 48: 'ष', 49: 'स', 50: 'ह', 51: '़', 52: 'ा', 53: 'ि', 54: 'ी', 55: 'ु', 56: 'ू', 57: 'ृ', 58: 'ॅ', 59: 'े', 60: 'ै', 61: 'ॉ', 62: 'ो', 63: 'ौ', 64: '्', 65: 'ॐ'}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def one_hot_encoding(input_words, target_words):\n",
        "\n",
        "    length = len(input_words)\n",
        "    encoder_input_array = np.zeros(\n",
        "        (length, max_encoder_seq_length, num_input_tokens+1), dtype=\"float32\"\n",
        "    )\n",
        "    decoder_input_array = np.zeros(\n",
        "        (length, max_decoder_seq_length, num_target_tokens+1), dtype=\"float32\"\n",
        "    )\n",
        "    decoder_output_array = np.zeros(\n",
        "        (length, max_decoder_seq_length, num_target_tokens+1), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "\n",
        "    for i, (input_text, target_text) in enumerate(zip(input_words, target_words)):\n",
        "        for t, char in enumerate(input_text):\n",
        "            encoder_input_array[i, t, input_char_map[char]] = 1.0\n",
        "        \n",
        "        for t, char in enumerate(target_text):\n",
        "            \n",
        "            decoder_input_array[i, t, target_char_map[char]] = 1.0\n",
        "            if t >=1 :\n",
        "                \n",
        "                decoder_output_array[i, t - 1, target_char_map[char]] = 1.0\n",
        "        \n",
        "    return(encoder_input_array,decoder_input_array,decoder_output_array)\n",
        "\n",
        "encoder_input_array, decoder_input_array, decoder_output_array = one_hot_encoding(input_words,target_words)\n",
        "val_encoder_input_array, val_decoder_input_array, val_decoder_output_array = one_hot_encoding(val_input_words,val_target_words)\n",
        "test_encoder_input_array, test_decoder_input_array, test_decoder_output_array = one_hot_encoding(test_input_words,test_target_words)\n",
        "\n",
        "print(decoder_input_array.shape)\n",
        "encoder_input_array = np.argmax(encoder_input_array, axis=2)\n",
        "decoder_input_array = np.argmax(decoder_input_array, axis=2)\n",
        "\n",
        "val_encoder_input_array = np.argmax(val_encoder_input_array, axis=2)\n",
        "test_encoder_input_array = np.argmax(test_encoder_input_array, axis=2)\n",
        "\n",
        "val_decoder_input_array = np.argmax(val_decoder_input_array, axis=2)\n",
        "test_decoder_input_array = np.argmax(test_decoder_input_array, axis=2)\n",
        "\n",
        "reverse_input_char_map = dict((i, char) for char, i in input_char_map.items())\n",
        "print(reverse_input_char_map)\n",
        "reverse_target_char_map = dict((i, char) for char, i in target_char_map.items())\n",
        "print(reverse_target_char_map)\n",
        "reverse_target_char_map[0] = \"\\n\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pS8bmTl4o4Oo"
      },
      "outputs": [],
      "source": [
        "def define_model(num_cells, cell_type, num_encoder_layers, num_decoder_layers, input_embedding_size, dropout_fraction, beam_size):\n",
        "    \n",
        "    encoder_input = keras.Input(shape=(None, ), name=\"enc_input\")\n",
        "    encoder_embedding = keras.layers.Embedding(num_input_tokens + 1, input_embedding_size, name=\"enc_embedding\", mask_zero=True)(encoder_input)\n",
        "\n",
        "    \n",
        "    states = {}\n",
        "    for i in range(0, num_encoder_layers):\n",
        "        if cell_type==\"LSTM\":\n",
        "\n",
        "            encoder = keras.layers.LSTM(num_cells, return_state=True, return_sequences=True, name=\"enc_\"+str(i+1), dropout=dropout_fraction, recurrent_dropout=dropout_fraction)\n",
        "\n",
        "            if i==0:\n",
        "                encoder_outputs, encoder_state_h, encoder_state_c = encoder(encoder_embedding)\n",
        "            else:\n",
        "                encoder_outputs, encoder_state_h, encoder_state_c = encoder(encoder_outputs)\n",
        "\n",
        "            states['encoder_state_h_'+str(i+1)] =  encoder_state_h\n",
        "            states['encoder_state_c_'+str(i+1)] =  encoder_state_c\n",
        "              \n",
        "\n",
        "        if cell_type==\"RNN\":\n",
        "  \n",
        "            encoder = keras.layers.SimpleRNN(num_cells, return_state=True, return_sequences=True, name=\"enc_\"+str(i+1), dropout=dropout_fraction, recurrent_dropout=dropout_fraction)\n",
        "            \n",
        "            if i==0:\n",
        "                whole_sequence_output, rnn_final_state = encoder(encoder_embedding)\n",
        "            else:\n",
        "                whole_sequence_output, rnn_final_state = encoder(whole_sequence_output)\n",
        "\n",
        "            states['rnn_final_state_'+str(i+1)] =  rnn_final_state\n",
        "            \n",
        "\n",
        "        if cell_type==\"GRU\":\n",
        "            \n",
        "            encoder = keras.layers.GRU(num_cells, return_state=True, return_sequences=True, name=\"enc_\"+str(i+1), dropout=dropout_fraction, recurrent_dropout=dropout_fraction)\n",
        "            \n",
        "            if i==0:\n",
        "                whole_sequence_output, gru_final_state = encoder(encoder_embedding)\n",
        "            else:\n",
        "                whole_sequence_output, gru_final_state = encoder(whole_sequence_output)\n",
        "\n",
        "            states['gru_final_state_'+str(i+1)] =  gru_final_state\n",
        "            \n",
        "\n",
        "   \n",
        "    decoder_input = keras.Input(shape=(None, ), name=\"dec_input\")\n",
        "    decoder_embedding = keras.layers.Embedding(num_target_tokens + 1, 64, name=\"dec_embedding\", mask_zero=True)(decoder_input)\n",
        "\n",
        "\n",
        "    for i in range(0, num_decoder_layers):\n",
        "        if cell_type==\"LSTM\":\n",
        "            decoder_lstm = keras.layers.LSTM(num_cells, return_sequences=True, return_state=True, name=\"dec_\"+str(i+1), dropout=dropout_fraction, recurrent_dropout=dropout_fraction)\n",
        "            \n",
        "            if i==0:\n",
        "                decoder_outputs, decoder_state_h, decoder_state_c = decoder_lstm(decoder_embedding, initial_state = [states['encoder_state_h_'+str(i+1)], states['encoder_state_c_'+str(i+1)]])\n",
        "            else:\n",
        "                decoder_outputs, decoder_state_h, decoder_state_c = decoder_lstm(decoder_outputs, initial_state = [states['encoder_state_h_'+str(i+1)],states['encoder_state_c_'+str(i+1)]])\n",
        "            \n",
        "\n",
        "        if cell_type==\"RNN\":\n",
        "            decoder_rnn = keras.layers.SimpleRNN(num_cells, return_sequences=True, return_state=True, name=\"dec_\"+str(i+1), dropout=dropout_fraction, recurrent_dropout=dropout_fraction)\n",
        "            if i==0:\n",
        "                decoder_outputs, rnn_decoder_final_state = decoder_rnn(decoder_embedding, initial_state = states['rnn_final_state_'+str(i+1)])\n",
        "            else:\n",
        "                decoder_outputs, rnn_decoder_final_state = decoder_rnn(decoder_outputs, initial_state = states['rnn_final_state_'+str(i+1)])\n",
        "            \n",
        "        if cell_type==\"GRU\":\n",
        "            decoder_gru = keras.layers.GRU(num_cells, return_sequences=True, return_state=True, name=\"dec_\"+str(i+1), dropout=dropout_fraction, recurrent_dropout=dropout_fraction)\n",
        "            if i==0:\n",
        "                decoder_outputs, gru_decoder_final_state = decoder_gru(decoder_embedding, initial_state = states['gru_final_state_'+str(i+1)])\n",
        "            else:\n",
        "                decoder_outputs, gru_decoder_final_state = decoder_gru(decoder_outputs, initial_state = states['gru_final_state_'+str(i+1)])\n",
        "            \n",
        "\n",
        "\n",
        "    decoder_dense = keras.layers.Dense(num_target_tokens + 1, activation=\"softmax\", name=\"dec_dense\") # Softmax picks one character\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "\n",
        "    model = keras.Model([encoder_input, decoder_input], decoder_outputs)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def inferenceLSTM(model, num_cells):\n",
        "   \n",
        "    \n",
        "    states={}\n",
        "    enc_states=[]\n",
        "    enc_inputs = model.input[0]\n",
        "    dec_inputs = model.input[1]\n",
        "\n",
        "    \n",
        "    for layer in model.layers:\n",
        "        string = layer.name\n",
        "        i= string[-1]\n",
        "        if(i.isnumeric() and string[0]=='e'):\n",
        "          _, enc_h_state, enc_c_state= layer.output\n",
        "          states['enc_h_state_'+i]=enc_h_state\n",
        "          states['enc_c_state_'+i]=enc_c_state\n",
        "          enc_states.append(states['enc_h_state_'+ i])\n",
        "          enc_states.append(states['enc_c_state_'+ i])\n",
        "  \n",
        "\n",
        "    enc_model = keras.Model(enc_inputs, enc_states)\n",
        "\n",
        "   \n",
        "    decoders={}\n",
        "    count=0\n",
        "    for layer in model.layers:\n",
        "        if layer.name==\"dec_dense\":\n",
        "            dec_dense = layer\n",
        "        if layer.name == \"dec_embedding\":\n",
        "            dec_embedding = layer\n",
        "        string = layer.name\n",
        "        i= string[-1]\n",
        "        if(i.isnumeric() and string[0]=='d'):\n",
        "          count+=1\n",
        "          decoders['decoder_'+i]=layer\n",
        "     \n",
        "\n",
        "    for i in range(1,count+1):\n",
        "      input_dec_h_state = keras.Input(shape=(num_cells,))\n",
        "      input_dec_c_state = keras.Input(shape=(num_cells,))\n",
        "      states['input_dec_h_state_'+str(i)]=input_dec_h_state\n",
        "      states['input_dec_c_state_'+str(i)]=input_dec_c_state\n",
        "\n",
        "\n",
        "\n",
        "    dec_states_inputs=[]\n",
        "    for i in range(1,count+1):\n",
        "      states['input_dec_states_'+str(i)]=[]\n",
        "      states['input_dec_states_'+str(i)].append(states['input_dec_h_state_'+str(i)])\n",
        "      states['input_dec_states_'+str(i)].append(states['input_dec_c_state_'+str(i)])\n",
        "      dec_states_inputs= dec_states_inputs+states['input_dec_states_'+str(i)]\n",
        "\n",
        "\n",
        "\n",
        "    dec_states=[]\n",
        "    for i in range(1,count+1):\n",
        "      if(i==1):\n",
        "        dec_outputs, dec_h_state, dec_c_state = decoders['decoder_'+str(i)](dec_embedding(dec_inputs), states['input_dec_states_'+str(i)])\n",
        "      else:\n",
        "        dec_outputs, dec_h_state, dec_c_state = decoders['decoder_'+str(i)](dec_outputs, states['input_dec_states_'+str(i)])\n",
        "      \n",
        "      states['dec_h_state_'+str(i)]= dec_h_state\n",
        "      states['dec_c_state_'+str(i)]= dec_c_state\n",
        "\n",
        "      dec_states.append(states['dec_h_state_'+str(i)])\n",
        "      dec_states.append(states['dec_c_state_'+str(i)])\n",
        "\n",
        "\n",
        "   \n",
        "\n",
        "   \n",
        "    dec_outputs = dec_dense(dec_outputs)\n",
        "   \n",
        "    dec_model = keras.Model([dec_inputs] + dec_states_inputs, [dec_outputs] + dec_states)\n",
        "\n",
        "    return enc_model, dec_model\n"
      ],
      "metadata": {
        "id": "WesgtPgqTmFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inferenceOther(model, num_cells):\n",
        "    \n",
        "    \n",
        "    states={}\n",
        "    enc_states=[]\n",
        "    enc_inputs = model.input[0]\n",
        "    dec_inputs = model.input[1]\n",
        "\n",
        "    \n",
        "    for layer in model.layers:\n",
        "        string = layer.name\n",
        "        i= string[-1]\n",
        "        if(i.isnumeric() and string[0]=='e'):\n",
        "          _, enc_state= layer.output\n",
        "          states['enc_state_'+i]= enc_state\n",
        "          enc_states.append(states['enc_state_'+ i])\n",
        "          \n",
        "  \n",
        "\n",
        "    \n",
        "    enc_model = keras.Model(enc_inputs, enc_states)\n",
        "\n",
        "   \n",
        "\n",
        "   \n",
        "    decoders={}\n",
        "    count=0\n",
        "    for layer in model.layers:\n",
        "        if layer.name==\"dec_dense\":\n",
        "            dec_dense = layer\n",
        "        if layer.name == \"dec_embedding\":\n",
        "            dec_embedding = layer\n",
        "        string = layer.name\n",
        "        i= string[-1]\n",
        "        if(i.isnumeric() and string[0]=='d'):\n",
        "          count+=1\n",
        "          decoders['decoder_'+i]=layer\n",
        "        \n",
        "    \n",
        "\n",
        "    for i in range(1,count+1):\n",
        "      input_dec_state = keras.Input(shape=(num_cells,))\n",
        "      states['input_dec_state_'+str(i)]=input_dec_state\n",
        "      \n",
        "\n",
        "    \n",
        "\n",
        "    dec_states_inputs=[]\n",
        "    for i in range(1,count+1):\n",
        "      states['input_dec_states_'+str(i)]=[]\n",
        "      states['input_dec_states_'+str(i)].append(states['input_dec_state_'+str(i)])\n",
        "      dec_states_inputs= dec_states_inputs+states['input_dec_states_'+str(i)]\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    dec_states=[]\n",
        "    for i in range(1,count+1):\n",
        "      if(i==1):\n",
        "        dec_outputs, dec_state = decoders['decoder_'+str(i)](dec_embedding(dec_inputs), states['input_dec_states_'+str(i)])\n",
        "      else:\n",
        "        dec_outputs, dec_state = decoders['decoder_'+str(i)](dec_outputs, states['input_dec_states_'+str(i)])\n",
        "      \n",
        "      states['dec_state_'+str(i)]= dec_state\n",
        "      \n",
        "\n",
        "      dec_states.append(states['dec_state_'+str(i)])\n",
        " \n",
        "\n",
        "  \n",
        "    dec_outputs = dec_dense(dec_outputs)\n",
        "   \n",
        "   \n",
        "    dec_model = keras.Model([dec_inputs] + dec_states_inputs, [dec_outputs] + dec_states)\n",
        "\n",
        "    \n",
        "    return enc_model, dec_model\n"
      ],
      "metadata": {
        "id": "waJWLvgT4p58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_words(input_words, enc_model, dec_model):\n",
        "    \n",
        "    batch_size = input_words.shape[0]\n",
        "    \n",
        "    enc_hidden_states = enc_model.predict(input_words)\n",
        "\n",
        "    target_sequence = np.zeros((batch_size, 1, num_target_tokens+1))\n",
        "    \n",
        "    target_sequence[:, 0, target_char_map[\"\\t\"]] = 1.0\n",
        "    target_sequence = np.argmax(target_sequence, axis=2)\n",
        "\n",
        "    dec_words=[]\n",
        "    for i in range(batch_size):\n",
        "      dec_words.append(\"\")\n",
        "\n",
        "   \n",
        "\n",
        "    for i in range(max_decoder_seq_length):\n",
        "\n",
        "        outputs = dec_model.predict([target_sequence] + enc_hidden_states)\n",
        "\n",
        "        outputs = list(outputs)\n",
        "\n",
        "        output_tokens = outputs[0]\n",
        "\n",
        "\n",
        "        sampled_char_indices = np.argmax(output_tokens[:, -1, :], axis=1)\n",
        "\n",
        "        enc_hidden_states=[]\n",
        "        \n",
        "        target_sequence = np.zeros((batch_size, 1, num_target_tokens+1))\n",
        "\n",
        "        for j, ch_index in enumerate(sampled_char_indices):\n",
        "            dec_words[j] += reverse_target_char_map[ch_index]\n",
        "            target_sequence[j, 0, ch_index] = 1.0\n",
        "\n",
        "        target_sequence = np.argmax(target_sequence, axis=2)\n",
        "\n",
        "        \n",
        "        \n",
        "        for i in range(1,len(outputs)):\n",
        "          enc_hidden_states.append(outputs[i]) \n",
        "\n",
        "    i=0\n",
        "    for word in dec_words:\n",
        "      dec_words[i] = word[:word.find(\"\\n\")]\n",
        "      i=i+1\n",
        "    \n",
        "    \n",
        "    \n",
        "    return dec_words"
      ],
      "metadata": {
        "id": "qTRpB3_KsXft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "  'name': 'Attention',\n",
        "  'method': 'bayes',\n",
        "  'metric': {\n",
        "      'name': 'accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "  'parameters': {\n",
        "      \n",
        "        'input_embedding':{\n",
        "            'values' : [32, 64, 128]\n",
        "        },\n",
        "        'enc_layers':{\n",
        "            'values':[1,2,3]\n",
        "        },\n",
        "        'dec_layers':{\n",
        "            'values':[1,2,3]\n",
        "        },\n",
        "        'hidden':{\n",
        "            'values':[64,128,256]\n",
        "        },\n",
        "        'cell_type':{\n",
        "            'values':['GRU', 'LSTM','RNN']\n",
        "        },\n",
        "        'dropout':{\n",
        "            'values':[0.0,0.3]\n",
        "        },\n",
        "        'epochs':{\n",
        "            'values':[5,10,15,20]\n",
        "        },\n",
        "        'rec_dropout':{\n",
        "            'values':[0.0,0.3]\n",
        "        },\n",
        "        'beam_size':{\n",
        "            'values':[1,3]\n",
        "        }\n",
        "\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project='CS6910 Assignment 3', entity='go4rav')"
      ],
      "metadata": {
        "id": "z5owCdL2tz6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXfqYiFMo-uJ"
      },
      "outputs": [],
      "source": [
        "def train(num_cells, cell_type, num_layers, input_embedding_size, dropout_fraction, beam_size, recurrent_dropout, epochs):\n",
        "   \n",
        "\n",
        "    model = define_model(num_cells, cell_type, num_layers, num_layers, input_embedding_size, dropout_fraction, beam_size)\n",
        "    print(model.summary())\n",
        "\n",
        "   \n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    \n",
        "    history = model.fit(\n",
        "            [encoder_input_array, decoder_input_array],\n",
        "            decoder_output_array,\n",
        "            batch_size = 64,\n",
        "            epochs = epochs,\n",
        "            verbose = 1,\n",
        "            validation_data = ([val_encoder_input_array, val_decoder_input_array], val_decoder_output_array),\n",
        "            callbacks=[WandbCallback()]\n",
        "            )\n",
        "    \n",
        "    \n",
        "    model.save(\"best_model_without_attention.h5\")\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "    if cell_type == \"LSTM\":\n",
        "        encoder_model, decoder_model = InferenceLSTM(model, num_cells)\n",
        "    else:\n",
        "        encoder_model, decoder_model = InferenceOther(model, num_cells)\n",
        "\n",
        "    \n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "    outputs = []\n",
        "    n = encoder_input_array.shape[0]\n",
        "    batch_size = 1000\n",
        "    for i in range(0, n, batch_size):\n",
        "        \n",
        "        query = encoder_input_array[i:i+batch_size]\n",
        "        \n",
        "        decoded_words = decode_words(query, encoder_model, decoder_model)\n",
        "        outputs = outputs + decoded_words\n",
        "\n",
        "   \n",
        "    ground_truths = [word[1:-1] for word in target_words]\n",
        "    \n",
        "    training_inference_accuracy = np.mean(np.array(outputs) == np.array(ground_truths))\n",
        "    \n",
        "\n",
        "    outputs = []\n",
        "    n = val_encoder_input_array.shape[0]\n",
        "    batch_size = 1000\n",
        "    for i in range(0, n, batch_size):\n",
        "       \n",
        "        query = val_encoder_input_array[i:i+batch_size]\n",
        "       \n",
        "        decoded_words = decode_words(query, encoder_model, decoder_model)\n",
        "        outputs = outputs + decoded_words\n",
        "\n",
        "  \n",
        "    ground_truths = [word[1:-1] for word in val_target_words]\n",
        "    \n",
        "    validation_inference_accuracy = np.mean(np.array(outputs) == np.array(ground_truths))\n",
        "   \n",
        "    \n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def model_train():\n",
        "\n",
        "  run = wandb.init()\n",
        "  configuration=run.config\n",
        "\n",
        "  wandb.run.name = ('model='\n",
        "        +str(configuration.cell_type)\n",
        "        +'_embed_size='\n",
        "        +str(configuration.input_embedding)\n",
        "        + '_num_enc='\n",
        "        + str(configuration.enc_layers)\n",
        "        + '_num_dec='\n",
        "        + str(configuration.dec_layers)\n",
        "        + \"_rec_drp_out=\"\n",
        "        + str(configuration.rec_dropout) \n",
        "        + \"_drp_out=\"\n",
        "        + str(configuration.dropout) \n",
        "        + \"_bm_size=\"\n",
        "        + str(configuration.beam_size)\n",
        "        + \"_hid_size=\"\n",
        "        + str(configuration.hidden)\n",
        "        + \"_epchs=\"\n",
        "        + str(configuration.epochs)\n",
        "    )\n",
        "  configuration.dec_layers = configuration.enc_layers\n",
        "\n",
        "  model, history = train(configuration.hidden, configuration.cell_type, configuration.dec_layers,configuration.input_embedding ,configuration.dropout, configuration.beam_size,configuration.rec_dropout,configuration.epochs)\n",
        "  \n"
      ],
      "metadata": {
        "id": "nC-A6pyvSyPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZMJlBzJFUcVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RR_EgQzvTpnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zp8umqEopHd-"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDXsofkCpLQ3"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": " seq_to_seq_Without_Attention_sweeps.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}